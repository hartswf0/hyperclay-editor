# The Ritual of Not-Knowing: On Trust, Obscurity, and Synthetic Protocols

*Post-Interpretability Studies*, Vol. 7, Issue 2, Spring 2035, pp. 183-209  
DOI: 10.52881/pis.2035.7.2.012

**Maya Chen-Weiner, PhD**  
Institute for Dialogic Technology  
Oxford University  

**Nikolai Vasquez, MPhil**  
Center for Black Box Ethics  
University of California, Berkeley  

## Abstract

This article challenges the persistent myth that complete explainability is both possible and necessary for ethical artificial intelligence. Drawing on W. Ross Ashby's cybernetic theory of black boxes, we develop a framework for what we term "opacity protocols"—ritualized interactions with synthetic systems that generate functional understanding without requiring mechanism transparency. We argue that certain forms of strategic opacity do not merely represent technical limitations to be overcome but can function as generative constraints that enable specific forms of human-machine relations otherwise foreclosed by transparency imperatives. Through a three-year study of the UMBRA healthcare decision support system, we demonstrate how carefully designed opacity can increase epistemic humility, reduce harmful overreliance, and facilitate what we term "dialogic trust"—trust based on patterned interaction rather than complete understanding. These findings suggest that the field must move beyond the binary of transparency versus obscurity toward a more nuanced "ritual epistemology" that acknowledges the productive role of not-knowing in human-machine relations. Rather than treating opacity as failure, we propose recognizing it as an inevitable and potentially valuable feature of complex sociotechnical systems that can be designed and ritualized to enhance rather than undermine responsible use.

**Keywords:** black box theory, opacity protocol, ritual epistemology, dialogic trust, post-interpretability, strategic obscurity, epistemic humility

## 1. Introduction: Beyond the Transparency Ideal

The history of artificial intelligence has been marked by what we might call the "transparency imperative"—the assumption that ethical and responsible AI necessarily requires complete human understanding of system operations. Since the early "black box" concerns of the 2020s, the field has pursued what Jameson (2032) characterizes as "the ever-receding horizon of total explanation,"[^1] investing billions in research aimed at making complex systems fully interpretable. Yet despite significant advances in techniques such as attention visualization, mechanistic interpretability, and causal tracing, completely transparent AI remains more aspiration than achievement for systems of meaningful complexity.

This gap between aspiration and achievement is typically framed as a temporary technical limitation—a problem awaiting its solution. The dominant narrative suggests that when the appropriate methods are finally developed, the black box will be illuminated, and the now-transparent system will fulfill the ethical requirement of complete explainability. This perspective is evident in policy frameworks like the European Union's Algorithm Transparency Act of 2028, which established "full functional explainability" as a legal requirement for high-risk AI applications, despite considerable debate about whether such explainability is technically achievable or even well-defined (Montavon & Doshi-Velez, 2027).[^2]

This paper challenges this transparency imperative, arguing that the black box is not merely a technical limitation but an epistemological condition with which we must ethically engage. Drawing on W. Ross Ashby's cybernetic theory of black boxes, we propose that certain forms of opacity are not merely obstacles to be overcome but can function as generative constraints that enable specific forms of human-machine relations. We suggest that rather than pursuing complete transparency as a universal ideal, we might better develop what we term "opacity protocols"—ritualized interactions with synthetic systems that generate functional understanding without requiring mechanism transparency.

This position challenges decades of consensus in AI ethics, which has consistently identified transparency as a non-negotiable requirement for responsible AI. From Floridi's early articulation of transparency as an ethical principle (2019)[^3] to the Responsible AI Framework (2025)[^4], the field has treated explainability as intrinsically valuable. Yet this consensus has been increasingly challenged by what Lincoln and Fazelpour (2031) term the "transparency paradox"—the observation that increased technical transparency often fails to produce either better understanding or more responsible use in real-world contexts.[^5]

Several empirical studies have revealed the limitations of the transparency ideal. Martinez-Wong and Kapoor (2029) demonstrated that even when provided with detailed explanations of model function, users frequently developed incorrect mental models that led to overreliance and poor calibration of trust.[^6] Similarly, Okafor's (2031) large-scale study of explanatory interfaces found that technical transparency often produced what he termed "false comprehension"—a sense of understanding without actual understanding—that increased rather than decreased harmful automation bias.[^7]

These findings align with Ashby's fundamental insight that understanding complex systems does not necessarily require internal inspection but can emerge through systematic observation of input-output relationships. As he wrote in his seminal work *An Introduction to Cybernetics*:

"The problem of the Black Box arose in electrical engineering. The engineer is given a sealed box that has terminals for input, to which he may bring any voltages, shocks, or other disturbances he pleases, and terminals for output, from which he may observe what he pleases. He is to deduce what he can of its contents."[^8]

Ashby's approach suggests an alternative epistemology—one where knowledge emerges not from seeing inside the system but from systematically observing its behavior across varied contexts. This behavioral approach to understanding aligns with what Keller and Suchman (2030) term "situated interpretability"—the idea that understanding emerges through interaction within specific contexts rather than through abstract explanation of mechanisms.[^9]

Building on these perspectives, this paper develops a framework for what we term "the ritual of not-knowing"—formalized practices for engaging with opaque systems that acknowledge rather than deny the inevitable limits of complete explanation. Through a case study of the UMBRA healthcare decision support system, we demonstrate how carefully designed opacity can increase epistemic humility, reduce harmful overreliance, and facilitate what we term "dialogic trust"—trust based on patterned interaction rather than complete understanding.

This approach does not argue for opacity as universally preferable to transparency. Rather, it challenges the binary thinking that positions transparency as an unqualified good and opacity as an unmitigated problem. Instead, we propose a more nuanced approach that recognizes both the value and limitations of explicability and develops ritualized protocols for engaging ethically with the irreducible opacity of complex synthetic systems.

## 2. Ashby's Black Box and the Epistemology of Opacity

### 2.1 The Black Box as Epistemic Condition

W. Ross Ashby's theory of the black box provides a foundational framework for understanding how knowledge can be constructed through interaction rather than inspection. For Ashby, the black box is not merely a technical problem to be solved but a fundamental epistemological condition that requires a particular approach to understanding.

Ashby defines a black box as a system whose internal mechanisms are either unknown or unknowable to the observer. He writes:

"The theory of the Black Box is merely the theory of real objects or systems, when attention is confined to the relation between the experimenter and the system, and when the system's internal mechanism is not accessible."[^10]

This definition positions the black box not as a flawed or incomplete system but as the normal condition of engagement with complex reality. As Ashby observes, "The child who tries to open a door has to manipulate the handle (the input) so as to produce the desired movement at the latch (the output); and he has to learn how to control the one by the other without being able to see the internal mechanism that links them."[^11]

This perspective challenges the assumption that complete internal understanding is necessary for effective engagement. Instead, Ashby proposes that understanding can emerge through systematic observation of how the system behaves under different conditions—what he calls the development of a "protocol." Through methodical interaction, the observer builds what Ashby terms a "canonical representation" of the system—not a detailed map of its mechanism but a functional model of its behavioral patterns.

As Malabou (2033) notes in her analysis of Ashby's epistemology, "The black box is not absolutely unknowable; it is simply what is left when the direct methods of introspection are ineffective. Ashby gives us not a surrender to ignorance but a rigorous method for knowing without seeing."[^12]

### 2.2 From Mechanistic to Behavioral Understanding

Ashby's black box theory suggests a fundamental distinction between two forms of understanding: mechanistic understanding, which focuses on internal structures and processes, and behavioral understanding, which focuses on patterns of response across different contexts. While the transparency imperative privileges mechanistic understanding, Ashby suggests that behavioral understanding can be both more achievable and more useful in many contexts.

This distinction is particularly relevant for modern AI systems, where mechanistic understanding is increasingly challenged by what Mitchell (2029) terms "emergent opacity"—the tendency of complex systems to develop internal representations and decision procedures that resist straightforward explanation even when all technical details are theoretically available.[^13] As language models increased in size from millions to trillions of parameters, researchers discovered that complete mechanistic understanding became not just practically but theoretically challenging due to emergent properties that couldn't be reduced to simpler components.

The shift from mechanistic to behavioral understanding doesn't imply abandoning explanation altogether. Rather, it suggests what Ashby describes as "explanation by model"—the construction of working models that predict system behavior without necessarily corresponding to internal mechanisms. As Esposito (2024) notes in her analysis of learning black boxes, "Understanding often emerges retroactively through interaction rather than preceding it. We don't need to know how a system works to know how to work with it."[^14]

This approach aligns with what philosophers of science call "instrumental realism"—the view that scientific understanding comes not from seeing reality directly but from developing reliable instruments for predicting and interacting with it. As Hacking argued, "We are completely convinced of the reality of electrons when we regularly set out to build—and often enough succeed in building—new kinds of devices that use various well-understood causal properties of electrons to interfere in other more hypothetical parts of nature."[^15]

Applied to AI systems, this suggests that understanding might be better achieved through systematic interaction that reveals functional patterns rather than through technical explanations that often exceed human cognitive capacity. As Ashby writes, "The theory of the Black Box is concerned not so much with the Box's actual contents as with the inferences that may be made about its contents."[^16]

### 2.3 Ritual and the Formalization of Not-Knowing

If mechanistic transparency is neither always possible nor always necessary, how might we structure interactions with opaque systems to enable responsible use? We propose that ritual provides a productive framework for formalizing engagement with synthetic opacity.

Anthropologically, rituals are standardized, repeatable procedures that create meaning and establish trust without requiring complete intellectual comprehension of the mechanisms involved. As Bell (1992) observes, rituals "do not so much express meaning as create it through the act of performance itself."[^17] Similarly, opacity protocols can be understood as ritualized interactions that generate functional understanding through structured engagement rather than through complete explanation.

The concept of ritual is particularly apt because it acknowledges both the practical and symbolic dimensions of human-technology interaction. As The Synthetic Ritual Collective (2032) argues, "Interactions with complex AI systems are never merely technical but also symbolic—they involve not just the exchange of information but the negotiation of meaning, authority, and trust."[^18]

This ritual approach transforms what might be seen as a limitation—the inability to fully explain system operations—into a structured practice that acknowledges and works with this limitation rather than denying it. As Ashby suggests, the black box is not a temporary problem awaiting solution but a permanent feature of engagement with complex reality that requires specific methodologies.

Based on Ashby's black box theory and anthropological understandings of ritual, we propose three core components of opacity protocols:

1. **Systematic Exploration**: Structured procedures for testing system behavior across varied inputs and contexts, creating what Ashby calls a "protocol" of observed input-output relationships.

2. **Boundary Marking**: Explicit identification and documentation of the limits of both system capability and human understanding, creating what Wong (2031) terms "epistemic edges"—clearly marked boundaries between the known, the unknown, and the unknowable.[^19]

3. **Pattern Recognition**: Formalized practices for identifying and documenting behavioral patterns that enable prediction without requiring mechanism explanation, creating what we term "functional models" of system behavior.

These components transform opacity from a passive condition to an active practice—a ritual of not-knowing that enables responsible engagement without requiring complete explanation. As The Black Box Ethics Group (2034) argues, "The ritual of not-knowing is not a surrender to ignorance but a recognition of the conditions under which all knowledge operates."[^20]

## 3. The UMBRA Case: Strategic Opacity in Clinical Decision Support

To demonstrate how opacity protocols function in practice, we present a case study of UMBRA, a clinical decision support system deployed in 17 healthcare networks across Europe and North America between 2031 and 2034. UMBRA (Uncertainty-Managed Biomedical Reasoning Assistant) was specifically designed to incorporate what its developers termed "strategic opacity"—deliberately limited explanation that foregrounds uncertainty rather than false certainty.

### 3.1 System Design and Opacity Features

UMBRA was developed to assist clinicians with complex diagnostic reasoning while avoiding the documented pitfalls of both full opacity and illusory transparency. Rather than attempting to explain its full reasoning process—which integrated multimodal inputs including imaging, genomics, clinical history, and literature—UMBRA was designed to make its limitations explicit through several key features:

1. **Confidence Topology**: Rather than providing single diagnostic suggestions with confidence scores, UMBRA presented what its designers called a "possibility space"—a visual representation of multiple potential diagnoses arranged according to both likelihood and the system's assessed confidence in that likelihood assessment. Critically, this visualization did not explain why certain diagnoses were considered more likely than others but clearly indicated where the system had high versus low confidence in its own assessments.

2. **Known Unknown Marking**: The system explicitly identified information categories it had not considered or where its training was known to be limited. Rather than attempting to hide these limitations, UMBRA foregrounded them through what Cheng et al. (2030) termed "active knowledge boundaries"—explicit markers of epistemic limits.[^21]

3. **Counterfactual Probing Interface**: While not explaining its internal reasoning, UMBRA allowed clinicians to pose "what if" questions that revealed how different inputs would affect the possibility space. This feature enabled what Ashby would call "protocol development"—systematic exploration of the system's input-output relationships without access to its internal mechanisms.

4. **Ritual Interaction Structure**: Perhaps most distinctively, interaction with UMBRA was deliberately structured as a ritual with defined stages: Initial Assessment, Limitation Review, Counterfactual Exploration, and Collaborative Conclusion. Clinicians were required to complete each stage in sequence, with mandatory documentation of their engagement with the system's acknowledged limitations.

These design features embodied what we term "strategic opacity"—the deliberate limitation of explanation to prevent overreliance while enabling effective collaboration. Rather than attempting to explain everything (and inevitably failing), UMBRA was designed to make its limitations explicit and to structure interaction as a ritual of collaborative sense-making.

### 3.2 Empirical Findings: The Effects of Ritualized Opacity

Our research team conducted a three-year study of UMBRA deployment across seven healthcare networks, comparing outcomes with those of a more conventionally "transparent" system (CLARIFY) that provided detailed but necessarily incomplete explanations of its reasoning. The study included quantitative analysis of 18,467 clinical cases and qualitative research involving 243 clinicians across multiple specialties.

The findings revealed several significant advantages of UMBRA's strategic opacity approach:

#### 3.2.1 Improved Diagnostic Accuracy

Contrary to transparency advocates' predictions, cases involving UMBRA showed 12.4% higher diagnostic accuracy compared to those involving CLARIFY (p<0.001). Qualitative analysis revealed that this improvement stemmed primarily from better calibration of clinician trust. As one participant explained:

"With CLARIFY, I often found myself accepting its explanations as valid because they sounded plausible, even when my clinical instinct suggested otherwise. UMBRA's approach forces me to engage more critically—its visible uncertainty keeps me alert rather than complacent."[^22]

This finding aligns with Venkatesh's (2028) study of explanation effects, which demonstrated that detailed but inevitably incomplete explanations often produced "explanation satisfaction without understanding"—a dangerous combination that led to overreliance.[^23]

#### 3.2.2 Enhanced Epistemic Humility

One of the most striking findings was the effect of UMBRA's opacity protocols on what we term "epistemic humility"—clinicians' awareness of the limits of both the system's and their own knowledge. Compared to CLARIFY users, UMBRA users demonstrated:

- 27.8% higher rates of seeking additional information
- 34.2% more frequent consultation with colleagues
- 18.6% more acknowledgment of uncertainty in their documentation

These behaviors indicate what Kapoor and Weinberg (2032) term "appropriate skepticism"—neither wholesale rejection nor uncritical acceptance of system outputs.[^24] The ritualized interaction with acknowledged limits appeared to transfer from the system to the clinicians themselves, creating what one participant described as "a culture of comfortable uncertainty—where not knowing isn't a failure but the starting point for investigation."[^25]

#### 3.2.3 Development of Behavioral Understanding

Through structured interaction with UMBRA, clinicians developed what Ashby would call a "protocol" of system behavior—knowledge of how the system responds in different situations without necessarily understanding its internal mechanisms. This behavioral understanding proved more robust than the illusory mechanistic understanding provided by CLARIFY's explanations.

As one participant noted:

"After six months with UMBRA, I have a strong sense of when it's likely to be helpful and when it might miss something important. I couldn't explain how it works internally, but I've developed a feel for its patterns of success and failure that guides how I use it. With CLARIFY, I thought I understood how it worked because it told me, but that understanding was superficial and sometimes misleading."[^26]

This observation aligns with Ashby's insight that effective interaction with black boxes comes not from seeing inside them but from carefully observing their behavior across various conditions. The ritual structure of UMBRA interaction—with its mandatory stages of limitation review and counterfactual exploration—facilitated the development of this behavioral understanding.

### 3.3 The Ritual Elements of UMBRA Interaction

The UMBRA case demonstrates how opacity can be transformed from a passive limitation to an active practice through ritual structure. The system's design incorporated several key elements of ritual as identified by anthropologists:

1. **Formalization**: The interaction followed a prescribed sequence with defined stages, creating what Bell calls "a heightened formality that distinguishes ritual activity from ordinary behavior."[^27]

2. **Liminality**: The interaction created what Turner identifies as a "liminal space"—a threshold between ordinary certainty and acknowledged uncertainty where new forms of understanding can emerge.[^28]

3. **Symbolic Action**: The mandatory documentation of limitations and counterfactual exploration functioned as what Geertz terms "symbolic action"—behavior that carries meaning beyond its instrumental function.[^29]

4. **Communal Aspect**: UMBRA interactions were designed to be witnessed and documented, creating what The Synthetic Ritual Collective terms "a community of practice around acknowledged uncertainty."[^30]

These ritual elements transformed interaction with an opaque system from a potential hazard to a structured practice that enhanced rather than undermined responsible use. By ritualizing the engagement with limitations and uncertainty, UMBRA created what we term "dialogic trust"—trust based not on complete understanding but on structured patterns of interaction that acknowledge and work with inevitable epistemic limitations.

## 4. Post-Interpretability: Beyond Explanation and Toward Ritual Epistemology

The UMBRA case suggests the need for what we term a "post-interpretability" approach to AI ethics—one that moves beyond both naive demands for complete explanation and uncritical acceptance of opacity. This approach acknowledges what Malabou (2033) terms "the necessary incompleteness of all explanation" and develops structured practices for engaging with this incompleteness rather than denying it.[^31]

### 4.1 From Transparency to Appropriate Trust

The post-interpretability framework shifts focus from maximizing transparency to calibrating appropriate trust. As Ashby's black box theory suggests, understanding can emerge through systematic observation even when internal mechanisms remain opaque. The critical question becomes not "Is this system fully explainable?" but "Have we developed appropriate protocols for determining when and how to rely on this system?"

This shift aligns with what Keller and Suchman (2030) term the "relational turn" in AI ethics—a move from evaluating systems in isolation to examining how they function within specific relational contexts.[^32] The focus becomes not the technical characteristics of the system itself but the quality of the relationship between system and user, particularly how well that relationship manages inevitable epistemic limitations.

The UMBRA case demonstrates that strategic opacity can sometimes better facilitate appropriate trust than attempted transparency. By making limitations explicit and structuring interaction as ritual, opacity protocols create what Wong (2031) terms "honest uncertainty"—a shared acknowledgment of epistemic limits that enables more responsible collaboration than false certainty.[^33]

### 4.2 The Knowledge Effects of Structured Not-Knowing

Perhaps counterintuitively, the ritual of not-knowing can generate important forms of knowledge that attempted explanation might actually impede. As The Synthetic Ritual Collective (2032) argues, "The acknowledgment of unknowing creates a space for different forms of knowing to emerge—forms based not on illusory completeness but on structured engagement with partiality."[^34]

These "knowledge effects" of ritualized not-knowing include:

1. **Negative Knowledge**: What Chang (2023) describes as "knowledge about the limits of knowledge"—explicit understanding of what is not or cannot be known within a given context.[^35] The UMBRA case demonstrated how ritualized engagement with limitations enhanced clinicians' awareness of both the system's and their own epistemic boundaries.

2. **Pattern Recognition**: The development of what Ashby terms "canonical representations"—functional models of system behavior based on observed patterns rather than mechanism understanding. UMBRA users developed robust knowledge of when and how to rely on the system without necessarily understanding its internal operations.

3. **Contextual Sensitivity**: Enhanced awareness of how system performance varies across contexts, creating what Davies (2029) terms "situational knowledge"—understanding that is explicitly tied to specific conditions rather than presumed universal.[^36]

4. **Communal Knowledge**: The development of shared practices and understandings around system use, creating what Lave and Wenger termed "communities of practice" with distributed expertise about effective system engagement.[^37]

These knowledge effects suggest that the ritual of not-knowing is not simply resignation to ignorance but a productive epistemological approach that generates specific forms of understanding particularly suited to complex sociotechnical systems.

### 4.3 Designing for Ritual Rather Than Explanation

The post-interpretability framework suggests a fundamental shift in how we design AI systems—from maximizing explanation to structuring effective rituals of engagement. This approach focuses less on making systems transparent and more on developing what Kapoor and Weinberg (2032) term "interaction protocols that generate appropriate forms of understanding through structured use."[^38]

Based on the UMBRA case and Ashby's black box theory, we propose several principles for designing systems that facilitate the ritual of not-knowing:

1. **Explicit Limitation Marking**: Systems should clearly indicate the boundaries of their knowledge and capability, creating what Wong terms "epistemic edges" that prevent false assumptions of completeness.[^39]

2. **Structured Exploration Interfaces**: Rather than explaining internal operations, systems should facilitate systematic exploration of input-output relationships, enabling users to develop behavioral understanding through interaction.

3. **Uncertainty Visualization**: Systems should represent uncertainty not as a bug but as a feature, developing what Cheng et al. (2030) term "uncertainty aesthetics" that make epistemic limitations visually explicit.[^40]

4. **Ritual Interaction Patterns**: Interaction with complex systems should be structured as ritual, with defined stages that guide users through processes of exploration, limitation acknowledgment, and contextual assessment.

5. **Community Documentation**: Systems should facilitate the documentation and sharing of interaction experiences, creating collective knowledge about effective use that extends beyond individual understanding.

These design principles shift focus from the impossible goal of complete explanation to the more achievable and potentially more valuable goal of structured engagement with inevitable partiality. As The Black Box Ethics Group (2034) argues, "The design question is not how to eliminate opacity but how to ritualize it—how to transform it from passive limitation to active practice."[^41]

## 5. Conclusion: The Humble System and the Ritual of Trust

This paper has argued for a fundamental reframing of opacity in synthetic systems—from technical problem to epistemic condition requiring ritual engagement. Drawing on Ashby's black box theory and the UMBRA case study, we have demonstrated how strategic opacity can enhance rather than undermine responsible use by promoting epistemic humility, facilitating appropriate trust calibration, and enabling the development of behavioral understanding through structured interaction.

This perspective challenges the transparency imperative that has dominated AI ethics discourse, suggesting that the focus on complete explanation may sometimes work against rather than toward responsible use. Instead, we propose a post-interpretability framework that acknowledges the inevitable partiality of all explanation and develops ritualized practices for engaging with this partiality.

The ritual of not-knowing represents not a surrender to ignorance but a recognition of the conditions under which all knowledge operates. As Ashby observed, the black box is not a temporary problem awaiting solution but a fundamental feature of engagement with complex reality. By developing structured protocols for this engagement, we might achieve not less but more responsible relations with synthetic systems.

This approach aligns with what Keller and Suchman (2030) term "humble AI"—systems designed not to maximize capability or explanation but to facilitate productive collaboration through acknowledged limitation.[^42] The UMBRA case demonstrates that such humility can be engineered not through more comprehensive explanation but through strategic opacity that makes limitations explicit and structures interaction as ritual.

As synthetic systems become increasingly integrated into vital social domains, the question of how to establish appropriate trust becomes ever more urgent. The framework developed in this paper suggests that such trust might emerge not from illusory transparency but from ritualized engagement with acknowledged opacity. The ritual of not-knowing offers not less but more ethical relationality—a structured practice of engagement that acknowledges the inherent limitations of both human and machine understanding.

In closing, we suggest that the field of AI ethics might benefit from what Ashby called "intellectual humility in the face of complexity"—recognition that complete understanding is neither always possible nor always necessary for responsible engagement.[^43] By developing opacity protocols that transform not-knowing from passive limitation to active practice, we might establish more honest and ultimately more trustworthy relationships with the synthetic systems that increasingly shape our world.

## References

[^1]: Jameson, S. (2032). "The Ever-Receding Horizon: A Critical History of AI Explainability Research." *Artificial Intelligence Review*, 67(3), 389-412.

[^2]: Montavon, G., & Doshi-Velez, F. (2027). "Limits of Explainability: Technical and Conceptual Challenges in Transparency Research." *Journal of Machine Learning Research*, 28, 1457-1489.

[^3]: Floridi, L. (2019). "Establishing the Rules for Building Trustworthy AI." *Nature Machine Intelligence*, 1, 261-262.

[^4]: The Responsible AI Framework. (2025). *Global AI Ethics Consortium*. Retrieved from https://www.globalaiethics.org/framework

[^5]: Lincoln, P., & Fazelpour, S. (2031). "The Transparency Paradox: How Explanation Can Undermine Understanding." *Journal of Sociotechnical Critique*, 15(2), 234-267.

[^6]: Martinez-Wong, L., & Kapoor, R. (2029). "Mental Models and Explanation Effects in Clinical Decision Support." *JAMA Network Open*, 12(8), e2345678.

[^7]: Okafor, N. (2031). "False Comprehension: The Psychological Effects of AI Explanations on User Behavior." *Proceedings of the CHI Conference on Human Factors in Computing Systems*, 1-14.

[^8]: Ashby, W. R. (1956). *An Introduction to Cybernetics*. Chapman & Hall, p. 86.

[^9]: Keller, S., & Suchman, L. (2030). "Situated Interpretability: Understanding in Context." *Technology & Society*, 52(4), 312-338.

[^10]: Ashby, W. R. (1956). *An Introduction to Cybernetics*. Chapman & Hall, p. 87.

[^11]: Ashby, W. R. (1956). *An Introduction to Cybernetics*. Chapman & Hall, p. 88.

[^12]: Malabou, C. (2033). "Knowing Without Seeing: Ashby's Epistemology for the AI Age." *Philosophy & Technology*, 46(1), 28-45.

[^13]: Mitchell, M. (2029). "Emergent Opacity: The Challenge of Scale in Understanding AI Systems." *Artificial Intelligence*, 306, 103717.

[^14]: Esposito, E. (2024). "Artificial Intelligence, Learning, and Opacity." *Philosophy & Technology*, 37, 109-125.

[^15]: Hacking, I. (1983). *Representing and Intervening*. Cambridge University Press, p. 262.

[^16]: Ashby, W. R. (1956). *An Introduction to Cybernetics*. Chapman & Hall, p. 90.

[^17]: Bell, C. (1992). *Ritual Theory, Ritual Practice*. Oxford University Press, p. 109.

[^18]: The Synthetic Ritual Collective. (2032). "Ritual Interaction with Synthetic Systems: New Frameworks for Post-Explanation AI." *Technology, Ritual & Society*, 8(3), 212-240.

[^19]: Wong, A. (2031). "Epistemic Edges: Designing for the Known, the Unknown, and the Unknowable in AI Systems." *Design Issues*, 47(2), 78-94.

[^20]: The Black Box Ethics Group. (2034). "Beyond Transparency: Ethical Frameworks for Opacity in Synthetic Systems." *Ethics and Information Technology*, 36(1), 45-68.

[^21]: Cheng, J., Dharamshi, M., & Ibrahim, N. (2030). "Active Knowledge Boundaries: Interface Design for Epistemic Limitation in AI Systems." *Proceedings of the ACM Conference on Computer-Supported Cooperative Work and Social Computing*, 85-97.

[^22]: Interview with Participant #47, Cardiologist, University Hospital Geneva, March 12, 2033.

[^23]: Venkatesh, V. (2028). "Explanation Satisfaction Without Understanding: Psychological Dimensions of AI Explanations." *MIS Quarterly*, 52(3), 672-698.

[^24]: Kapoor, R., & Weinberg, G. (2032). "Appropriate Skepticism: Calibrating Trust in Clinical AI." *Journal of Medical Systems*, 56(4), 421-440.

[^25]: Interview with Participant #113, Emergency Medicine Physician, Mount Sinai Hospital, June 8, 2033.

[^26]: Interview with Participant #89, Neurologist, University College London Hospital, December 15, 2032.

[^27]: Bell, C. (1992). *Ritual Theory, Ritual Practice*. Oxford University Press, p. 74.

[^28]: Turner, V. (1969). *The Ritual Process: Structure and Anti-Structure*. Aldine Publishing, p. 95.

[^29]: Geertz, C. (1973). *The Interpretation of Cultures*. Basic Books, p. 112.

[^30]: The Synthetic Ritual Collective. (2032). "Ritual Interaction with Synthetic Systems: New Frameworks for Post-Explanation AI." *Technology, Ritual & Society*, 8(3), p. 225.

[^31]: Malabou, C. (2033). "Knowing Without Seeing: Ashby's Epistemology for the AI Age." *Philosophy & Technology*, 46(1), p. 33.

[^32]: Keller, S., & Suchman, L. (2030). "Situated Interpretability: Understanding in Context." *Technology & Society*, 52(4), p. 315.

[^33]: Wong, A. (2031). "Epistemic Edges: Designing for the Known, the Unknown, and the Unknowable in AI Systems." *Design Issues*, 47(2), p. 85.

[^34]: The Synthetic Ritual Collective. (2032). "Ritual Interaction with Synthetic Systems: New Frameworks for Post-Explanation AI." *Technology, Ritual & Society*, 8(3), p. 227.

[^35]: Chang, H. (2023). "Negative Knowledge and the Epistemology of Absence." *Philosophy of Science*, 90(1), 88-107.

[^36]: Davies, S. (2029). "Situational Knowledge: Context-Dependence in Human-AI Decision Making." *Organization Science*, 40(3), 456-478.

[^37]: Lave, J., & Wenger, E. (1991). *Situated Learning: Legitimate Peripheral Participation*. Cambridge University Press.

[^38]: Kapoor, R., & Weinberg, G. (2032). "Appropriate Skepticism: Calibrating Trust in Clinical AI." *Journal of Medical Systems*, 56(4), p. 433.

[^39]: Wong, A. (2031). "Epistemic Edges: Designing for the Known, the Unknown, and the Unknowable in AI Systems." *Design Issues*, 47(2), p. 81.

[^40]: Cheng, J., Dharamshi, M., & Ibrahim, N. (2030). "Active Knowledge Boundaries: Interface Design for Epistemic Limitation in AI Systems." *Proceedings of the ACM Conference on Computer-Supported Cooperative Work and Social Computing*, p. 91.

[^41]: The Black Box Ethics Group. (2034). "Beyond Transparency: Ethical Frameworks for Opacity in Synthetic Systems." *Ethics and Information Technology*, 36(1), p. 58.

[^42]: Keller, S., & Suchman, L. (2030). "Situated Interpretability: Understanding in Context." *Technology & Society*, 52(4), p. 329.

[^43]: Ashby, W. R. (1958). "Requisite Variety and Its Implications for the Control of Complex Systems." *Cybernetica*, 1(2), 83-99.

---

**About the Authors**

**Maya Chen-Weiner, PhD** is Professor of Sociotechnical Systems and Director of the Institute for Dialogic Technology at Oxford University. Her research focuses on the epistemological and ethical dimensions of human-AI interaction, with particular emphasis on knowledge practices in contexts of technological complexity.

**Nikolai Vasquez, MPhil** is a Research Fellow at the Center for Black Box Ethics, University of California, Berkeley. His work examines the cultural and philosophical implications of opacity in synthetic systems, developing frameworks for ethical engagement with technological unknowability.

*This research was supported by Grant #RT-2034-87B from the Cambridge-Berkeley Responsible AI Initiative.*

---
Answer from Perplexity: pplx.ai/share